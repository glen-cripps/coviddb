{"kernelspec":{"display_name":"Python 3 (system-wide)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}}
{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":"import re\nimport time\nimport json\nimport os\nimport pandas as pd\nimport arrow\ntoday_dt = arrow.now().format('YYYYMMDD')\n\n\n"}
{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":"news = pd.read_parquet(\"yahoo_most_viewed.\" + today_dt + \".parquet\")\n"}
{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::6d3e4332-c348-4b09-9c5a-94ca1225bc98","text/plain":"smc-blob::0e91b943-756e-4f41-892d-25eb923cfb99"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"news"}
{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":"def strip_links(text):\n    link_regex    = re.compile('((http?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], ', ')    \n    return text\n\ndef strip_all_entities(text):\n    import string\n    entity_prefixes = ['@','#']\n    for separator in  string.punctuation:\n        if separator not in entity_prefixes :\n            if separator == \"'\":\n                text = text.replace(separator,'')\n            else:\n                text = text.replace(separator,' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if (word[0] not in entity_prefixes) and (word != 'RT'):\n                words.append(word)    \n    return ' '.join(words)\n\ndef strip_numbers(text):\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ndef strip_junk(text):\n    text = re.sub(r'\\W+', ' ', text)\n    return text\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n# 7. Use stemming technique to return words backs to its root form (avoid using aggressive\n# stemming)\ndef stem_sentences(sentence):\n    tokens = sentence.split()\n    stemmed_tokens = [ps.stem(token) for token in tokens]\n    return ' '.join(stemmed_tokens)\n\n\ndef preprocess(text_data):\n    preprocessed_text=[]\n    for text in text_data:\n        #regext to remove RT retweets\n        text=re.sub('RT',\" \",text)\n        #regex to remove punctuations,hashtags,@usermentions,URLs and convert to lowercase\n        text=re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).lower()\n        #regex to remove numbers\n        text=re.sub(\"\\d+\", \" \", text)\n        preprocessed_text.append(text)\n    return preprocessed_text\ndef make_lower(text):\n    text = text.lower()\n    return text\n"}
{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":"\nnews['text_uni'] = news['text'].astype('unicode')\nnews['text_str'] = news['text_uni'].str.encode('ascii', 'ignore').str.decode('ascii')\n\n\nnews['text_clean'] = news['text_str'].apply(strip_links).apply(strip_all_entities) \\\n        .apply(strip_numbers).apply(make_lower) \\\n        .apply(strip_junk).apply(stem_sentences)\n\n"}
{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":"news.columns = news.columns.str.lower()\nnews['word_list'] = news['text_clean'].str.split()\n"}
{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":"# Vocabulary time\nvocab_dict = {}\nfor word_list in news['word_list']:\n    for word in word_list:\n        vocab_dict[word] = vocab_dict.get(word, 0) + 1\n# K now clean up that list by eliminating the stopwords"}
{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}],"source":"# 8. Remove stopwords by using from nltk.corpus import stopwords,\n# stopwords are dumb words like \"the\" and \"a\" \nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords"}
{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":"stopwords=set(stopwords.words('english'))\nvocab_clean_dict = {k:v for k,v in vocab_dict.items() if v>1 and k not in stopwords}\n"}
{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":"# The bag of words is too dam big and crashes the computer\n# So let's only allow words with 10 or more instances \nvocab_toosmall_dict = {k:v for k,v in vocab_clean_dict.items() if v<10}\nvocab_trimmed_dict = {k:v for k,v in vocab_clean_dict.items() if v>=10}\n"}
{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":"pat = r'\\b(?:{})\\b'.format('|'.join(vocab_toosmall_dict.keys()))\nnews['text_trim'] = news['text_clean'].str.replace(pat, '')\n"}
{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::93958722-7d8e-49bd-bf34-be9b6833f046","text/plain":"smc-blob::f7f10578-f950-444d-acd7-fb428ee63d89"},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":"pd.options.display.max_colwidth = 240\n\nnews[['text','text_trim']]\n\n\n"}
{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[],"source":"#  10 - Now, use the scikit-learn function CountVectorizer to create bag-of-words features        \nimport scipy.sparse\nfrom sklearn.feature_extraction.text import CountVectorizer\n"}
{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":"vectorizer=CountVectorizer()\nvectorizer.fit(vocab_trimmed_dict)\n\nnews['bag_of_words'] = list(vectorizer.transform(news['text_trim']).todense())\n\n\n"}
{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":"import nltk\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\n\ndef tokenizeit(text_data):\n    tokenized_text=[]\n    for text in text_data:\n        #tokenize the text\n        tokenized=nltk.word_tokenize(text)\n        #remove stop words\n        tokenized_text.append(\" \".join(list(x for x in tokenized if x not in stop)))\n    return tokenized_text\n\n\nnews['text_tokenized']=tokenizeit(news['text_trim'])\n"}
{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(45, 112)\n(45, 212)\n(45, 512)\n(45, 1012)\n(45, 1863)\n"}],"source":"for num_words in [100,200,500,1000,2000]: \n    vectorizer = CountVectorizer(max_features = num_words, stop_words='english')\n    bag_of_words = vectorizer.fit_transform(news['text_tokenized']).toarray()\n\n    sum_of_words=bag_of_words.sum(axis=0)\n\n    df=pd.DataFrame(bag_of_words,columns=vectorizer.get_feature_names())\n    \n    df.columns = ['bow_' + str(col) for col in df.columns]\n\n    #column names need to be str for parquet (apparently!)\n    df.columns = df.columns.map(str)\n\n#    df = tweets.append(df, ignore_index=True)\n    \n    df = pd.concat([news, df],axis=1)\n    \n    pretty_front = df[['text','text_tokenized']]\n    pretty_back = df.drop(['text','text_tokenized'], axis=1)\n    pretty_full = pd.concat([pretty_front,pretty_back], axis=1)\n\n    #write as parquet\n    pretty_full.drop(columns=['bag_of_words']).to_parquet(\"yahoo_most_viewed_bow_t\" + str(num_words) + \".\" + str(today_dt) + \".parquet\")\n\n    print(pretty_full.shape)"}
{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::1786a9ff-2f03-434a-a58c-5361a2deb889","text/plain":"smc-blob::b8857353-2a27-4e53-a31e-1f14b5e24118"},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":"pretty_full"}