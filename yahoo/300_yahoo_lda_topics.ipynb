{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import arrow\n",
    "today_dt = arrow.now().format('YYYYMMDD')\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_parquet(\"yahoo_most_viewed_bow_t100.\" + today_dt + \".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "news['nouns'] = news['word_list'] \n",
    "news['postags'] = news['word_list'].map(nltk.pos_tag)\n",
    "for idx, tags in news.iterrows():\n",
    "    tags = news['postags'][idx]\n",
    "    nouns = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    news['nouns'][idx] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nouns</th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[francisco, world, desper, news, coronaviru, d...</td>\n",
       "      <td>[san, francisco, in, a, world, desper, for, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[york, father, refus, age, son, nanuet, home, ...</td>\n",
       "      <td>[a, new, york, father, refus, to, let, hi, col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[world, tri, desper, coronaviru, pandem, brazi...</td>\n",
       "      <td>[as, the, world, tri, desper, to, tackl, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[brenda, goh, jingzhou, china, reuter, sit, cr...</td>\n",
       "      <td>[by, brenda, goh, jingzhou, china, reuter, ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[presid, fiddl, peopl, speaker, nanci, pelosi,...</td>\n",
       "      <td>[as, the, presid, fiddl, peopl, are, die, said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[news, primetim, star, tucker, carlson, credit...</td>\n",
       "      <td>[fox, news, primetim, star, tucker, carlson, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[rome, ap, andrea, didnt, profil, patient, hea...</td>\n",
       "      <td>[rome, ap, andrea, napoli, didnt, fit, the, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[trend, deploy, trend, deploy, n, type, mask, ...</td>\n",
       "      <td>[trend, deploy, onlin, retail, trend, deploy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[coronaviru, pandem, spread, presid, andr, man...</td>\n",
       "      <td>[as, the, coronaviru, pandem, spread, mexican,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[year, inmat, die, saturday, contract, coronav...</td>\n",
       "      <td>[a, year, old, inmat, die, saturday, after, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[tehran, afp, presid, rouhani, warn, sunday, w...</td>\n",
       "      <td>[tehran, afp, presid, hassan, rouhani, warn, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[minist, ha, ask, hi, countri, forgiv, impos, ...</td>\n",
       "      <td>[india, prime, minist, ha, ask, for, hi, count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[s, center, diseas, control, prevent, issu, re...</td>\n",
       "      <td>[the, u, s, center, for, diseas, control, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[stockholm, street, stockholm, peopl, cafe, ce...</td>\n",
       "      <td>[stockholm, ap, the, street, of, stockholm, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[chip, somodevilla, getti, depart, action, sto...</td>\n",
       "      <td>[chip, somodevilla, getti, imag, the, justic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[tornado, tore, citi, arkansa, flip, car, curf...</td>\n",
       "      <td>[a, tornado, tore, through, a, small, citi, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[eduardo, munoz, reuter, nurs, manhattan, hosp...</td>\n",
       "      <td>[eduardo, munoz, reuter, a, nurs, at, a, manha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[develop, asia, relat, coronaviru, pandem, wuh...</td>\n",
       "      <td>[here, are, the, latest, develop, from, asia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[hyonhe, seoul, reuter, korea, unit, state, le...</td>\n",
       "      <td>[by, hyonhe, shin, seoul, reuter, north, korea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[price, oil, ha, sunk, sinc, demand, collaps, ...</td>\n",
       "      <td>[the, price, of, oil, ha, sunk, to, level, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[vice, presid, joe, biden, democrat, nomin, be...</td>\n",
       "      <td>[if, former, vice, presid, joe, biden, secur, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[day, hospit, system, york, citi, visitor, pol...</td>\n",
       "      <td>[sever, day, ago, two, of, the, largest, hospi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[manila, philippin, plane, carri, peopl, inclu...</td>\n",
       "      <td>[manila, philippin, ap, a, plane, carri, eight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[manjunath, kiran, afp, getti, coronaviru, spr...</td>\n",
       "      <td>[manjunath, kiran, afp, via, getti, imag, a, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[burst, data, preval, silent, asymptomat, carr...</td>\n",
       "      <td>[a, burst, of, fresh, data, on, the, preval, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[dr, anthoni, fauci, unit, state, death, infec...</td>\n",
       "      <td>[dr, anthoni, fauci, say, the, unit, state, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[kremlin, shift, ownership, dollar, oil, proje...</td>\n",
       "      <td>[bloomberg, the, kremlin, sudden, shift, of, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[giulia, number, death, coronaviru, itali, day...</td>\n",
       "      <td>[by, giulia, segreti, rome, reuter, the, numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[miller, getti, imag, cinevega, parti, fine, s...</td>\n",
       "      <td>[ethan, miller, getti, imag, for, cinevega, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[trump, ha, gain, ground, hi, probabl, challen...</td>\n",
       "      <td>[donald, trump, ha, gain, ground, on, hi, prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[warn, user, care, ga, station, post, brent, m...</td>\n",
       "      <td>[a, warn, user, to, be, care, when, go, to, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[seoul, north, korea, fire, ballist, missil, s...</td>\n",
       "      <td>[seoul, south, korea, ap, north, korea, on, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[panama, citi, afp, passeng, viru, liner, amer...</td>\n",
       "      <td>[panama, citi, afp, passeng, on, a, viru, stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[merci, ha, arriv, angel, burden, hospit, numb...</td>\n",
       "      <td>[the, usn, merci, ha, arriv, in, lo, angel, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[coop, ward, zimbabwean, man, plead, hi, famil...</td>\n",
       "      <td>[coop, up, in, an, isol, ward, a, young, zimba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[berwick, matt, spetalnick, washington, reuter...</td>\n",
       "      <td>[by, angu, berwick, and, matt, spetalnick, car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[alyssa, burk, facebook, evan, clower, jonesbo...</td>\n",
       "      <td>[alyssa, burk, facebook, evan, clower, gofundm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[berni, sander, odd, democrat, howev, game, pl...</td>\n",
       "      <td>[berni, sander, is, is, far, from, over, even,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[stock, loss, america, week, coronaviru, pande...</td>\n",
       "      <td>[stock, were, project, to, extend, loss, again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[york, outbreak, coronaviru, ha, shock, econom...</td>\n",
       "      <td>[new, york, ap, the, outbreak, of, the, corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[berlin, afp, merkel, conserv, poll, thank, ha...</td>\n",
       "      <td>[berlin, afp, angela, merkel, long, struggl, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[detaine, detent, center, south, hand, crackdo...</td>\n",
       "      <td>[detaine, at, immigr, detent, center, across, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[govern, ha, handl, coronaviru, outbreak, lock...</td>\n",
       "      <td>[the, indian, govern, ha, defend, it, handl, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[day, plea, governor, countri, presid, trump, ...</td>\n",
       "      <td>[after, day, of, plea, from, governor, across,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[brenda, goh, thoma, suen, china, reuter, numb...</td>\n",
       "      <td>[by, brenda, goh, and, thoma, suen, wuhan, chi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                nouns  \\\n",
       "0   [francisco, world, desper, news, coronaviru, d...   \n",
       "1   [york, father, refus, age, son, nanuet, home, ...   \n",
       "2   [world, tri, desper, coronaviru, pandem, brazi...   \n",
       "3   [brenda, goh, jingzhou, china, reuter, sit, cr...   \n",
       "4   [presid, fiddl, peopl, speaker, nanci, pelosi,...   \n",
       "5   [news, primetim, star, tucker, carlson, credit...   \n",
       "6   [rome, ap, andrea, didnt, profil, patient, hea...   \n",
       "7   [trend, deploy, trend, deploy, n, type, mask, ...   \n",
       "8   [coronaviru, pandem, spread, presid, andr, man...   \n",
       "9   [year, inmat, die, saturday, contract, coronav...   \n",
       "10  [tehran, afp, presid, rouhani, warn, sunday, w...   \n",
       "11  [minist, ha, ask, hi, countri, forgiv, impos, ...   \n",
       "12  [s, center, diseas, control, prevent, issu, re...   \n",
       "13  [stockholm, street, stockholm, peopl, cafe, ce...   \n",
       "14  [chip, somodevilla, getti, depart, action, sto...   \n",
       "15  [tornado, tore, citi, arkansa, flip, car, curf...   \n",
       "16  [eduardo, munoz, reuter, nurs, manhattan, hosp...   \n",
       "17  [develop, asia, relat, coronaviru, pandem, wuh...   \n",
       "18  [hyonhe, seoul, reuter, korea, unit, state, le...   \n",
       "19  [price, oil, ha, sunk, sinc, demand, collaps, ...   \n",
       "20  [vice, presid, joe, biden, democrat, nomin, be...   \n",
       "21  [day, hospit, system, york, citi, visitor, pol...   \n",
       "22  [manila, philippin, plane, carri, peopl, inclu...   \n",
       "23  [manjunath, kiran, afp, getti, coronaviru, spr...   \n",
       "24  [burst, data, preval, silent, asymptomat, carr...   \n",
       "25  [dr, anthoni, fauci, unit, state, death, infec...   \n",
       "26  [kremlin, shift, ownership, dollar, oil, proje...   \n",
       "27  [giulia, number, death, coronaviru, itali, day...   \n",
       "28  [miller, getti, imag, cinevega, parti, fine, s...   \n",
       "29  [trump, ha, gain, ground, hi, probabl, challen...   \n",
       "30  [warn, user, care, ga, station, post, brent, m...   \n",
       "31  [seoul, north, korea, fire, ballist, missil, s...   \n",
       "32  [panama, citi, afp, passeng, viru, liner, amer...   \n",
       "33  [merci, ha, arriv, angel, burden, hospit, numb...   \n",
       "34  [coop, ward, zimbabwean, man, plead, hi, famil...   \n",
       "35  [berwick, matt, spetalnick, washington, reuter...   \n",
       "36  [alyssa, burk, facebook, evan, clower, jonesbo...   \n",
       "37  [berni, sander, odd, democrat, howev, game, pl...   \n",
       "38  [stock, loss, america, week, coronaviru, pande...   \n",
       "39  [york, outbreak, coronaviru, ha, shock, econom...   \n",
       "40  [berlin, afp, merkel, conserv, poll, thank, ha...   \n",
       "41  [detaine, detent, center, south, hand, crackdo...   \n",
       "42  [govern, ha, handl, coronaviru, outbreak, lock...   \n",
       "43  [day, plea, governor, countri, presid, trump, ...   \n",
       "44  [brenda, goh, thoma, suen, china, reuter, numb...   \n",
       "\n",
       "                                            word_list  \n",
       "0   [san, francisco, in, a, world, desper, for, go...  \n",
       "1   [a, new, york, father, refus, to, let, hi, col...  \n",
       "2   [as, the, world, tri, desper, to, tackl, the, ...  \n",
       "3   [by, brenda, goh, jingzhou, china, reuter, ten...  \n",
       "4   [as, the, presid, fiddl, peopl, are, die, said...  \n",
       "5   [fox, news, primetim, star, tucker, carlson, h...  \n",
       "6   [rome, ap, andrea, napoli, didnt, fit, the, us...  \n",
       "7   [trend, deploy, onlin, retail, trend, deploy, ...  \n",
       "8   [as, the, coronaviru, pandem, spread, mexican,...  \n",
       "9   [a, year, old, inmat, die, saturday, after, co...  \n",
       "10  [tehran, afp, presid, hassan, rouhani, warn, s...  \n",
       "11  [india, prime, minist, ha, ask, for, hi, count...  \n",
       "12  [the, u, s, center, for, diseas, control, and,...  \n",
       "13  [stockholm, ap, the, street, of, stockholm, ar...  \n",
       "14  [chip, somodevilla, getti, imag, the, justic, ...  \n",
       "15  [a, tornado, tore, through, a, small, citi, in...  \n",
       "16  [eduardo, munoz, reuter, a, nurs, at, a, manha...  \n",
       "17  [here, are, the, latest, develop, from, asia, ...  \n",
       "18  [by, hyonhe, shin, seoul, reuter, north, korea...  \n",
       "19  [the, price, of, oil, ha, sunk, to, level, not...  \n",
       "20  [if, former, vice, presid, joe, biden, secur, ...  \n",
       "21  [sever, day, ago, two, of, the, largest, hospi...  \n",
       "22  [manila, philippin, ap, a, plane, carri, eight...  \n",
       "23  [manjunath, kiran, afp, via, getti, imag, a, c...  \n",
       "24  [a, burst, of, fresh, data, on, the, preval, o...  \n",
       "25  [dr, anthoni, fauci, say, the, unit, state, co...  \n",
       "26  [bloomberg, the, kremlin, sudden, shift, of, o...  \n",
       "27  [by, giulia, segreti, rome, reuter, the, numbe...  \n",
       "28  [ethan, miller, getti, imag, for, cinevega, we...  \n",
       "29  [donald, trump, ha, gain, ground, on, hi, prob...  \n",
       "30  [a, warn, user, to, be, care, when, go, to, th...  \n",
       "31  [seoul, south, korea, ap, north, korea, on, su...  \n",
       "32  [panama, citi, afp, passeng, on, a, viru, stri...  \n",
       "33  [the, usn, merci, ha, arriv, in, lo, angel, to...  \n",
       "34  [coop, up, in, an, isol, ward, a, young, zimba...  \n",
       "35  [by, angu, berwick, and, matt, spetalnick, car...  \n",
       "36  [alyssa, burk, facebook, evan, clower, gofundm...  \n",
       "37  [berni, sander, is, is, far, from, over, even,...  \n",
       "38  [stock, were, project, to, extend, loss, again...  \n",
       "39  [new, york, ap, the, outbreak, of, the, corona...  \n",
       "40  [berlin, afp, angela, merkel, long, struggl, c...  \n",
       "41  [detaine, at, immigr, detent, center, across, ...  \n",
       "42  [the, indian, govern, ha, defend, it, handl, o...  \n",
       "43  [after, day, of, plea, from, governor, across,...  \n",
       "44  [by, brenda, goh, and, thoma, suen, wuhan, chi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[['nouns', 'word_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['francisco',\n",
       "  'world',\n",
       "  'desper',\n",
       "  'news',\n",
       "  'coronaviru',\n",
       "  'dip',\n",
       "  'carbon',\n",
       "  'caus',\n",
       "  'downturn',\n",
       "  'line',\n",
       "  'scientist',\n",
       "  'polici',\n",
       "  'expert',\n",
       "  'encourag',\n",
       "  'oil',\n",
       "  'demand',\n",
       "  'ha',\n",
       "  'percent',\n",
       "  'economi',\n",
       "  'world',\n",
       "  'halt',\n",
       "  'car',\n",
       "  'truck',\n",
       "  'jet',\n",
       "  'leav',\n",
       "  'contrail',\n",
       "  'sky',\n",
       "  'compil',\n",
       "  'diego',\n",
       "  'scripp',\n",
       "  'institut',\n",
       "  'rate',\n",
       "  'rise',\n",
       "  'carbon',\n",
       "  'dioxid',\n",
       "  'earth',\n",
       "  'slow',\n",
       "  'decad',\n",
       "  'warn',\n",
       "  'world',\n",
       "  'need',\n",
       "  'model',\n",
       "  'howev',\n",
       "  'thi',\n",
       "  'way',\n",
       "  'rob',\n",
       "  'jackson',\n",
       "  'professor',\n",
       "  'earth',\n",
       "  'system',\n",
       "  'scienc',\n",
       "  'stanford',\n",
       "  'univers',\n",
       "  'world',\n",
       "  'lead',\n",
       "  'expert',\n",
       "  'climat',\n",
       "  'chang',\n",
       "  'news',\n",
       "  'hundr',\n",
       "  'peopl',\n",
       "  'work',\n",
       "  'tool',\n",
       "  'curtail',\n",
       "  'econom',\n",
       "  'activ',\n",
       "  'jackson',\n",
       "  'author',\n",
       "  'world',\n",
       "  'meteorolog',\n",
       "  'organ',\n",
       "  'hi',\n",
       "  'colleagu',\n",
       "  'warn',\n",
       "  'need',\n",
       "  'action',\n",
       "  'carbon',\n",
       "  'goal',\n",
       "  'pari',\n",
       "  'agreement',\n",
       "  'averag',\n",
       "  'temperatur',\n",
       "  'rise',\n",
       "  'degre',\n",
       "  'celsiu',\n",
       "  'abov',\n",
       "  'level',\n",
       "  'rel',\n",
       "  'dip',\n",
       "  'carbon',\n",
       "  'emiss',\n",
       "  'econom',\n",
       "  'disrupt',\n",
       "  'caus',\n",
       "  'coronaviru',\n",
       "  'pandem',\n",
       "  'jackson',\n",
       "  'impact',\n",
       "  'degre',\n",
       "  'percent',\n",
       "  'year',\n",
       "  'jackson',\n",
       "  'imposs',\n",
       "  'complet',\n",
       "  'shutdown',\n",
       "  'interest',\n",
       "  'percent',\n",
       "  'year',\n",
       "  'year',\n",
       "  'doubt',\n",
       "  'reason',\n",
       "  'christi',\n",
       "  'goldfuss',\n",
       "  'manag',\n",
       "  'director',\n",
       "  'council',\n",
       "  'environment',\n",
       "  'qualiti',\n",
       "  'dure',\n",
       "  'obama',\n",
       "  'administr',\n",
       "  'agre',\n",
       "  'slowdown',\n",
       "  'coronaviru',\n",
       "  'outbreak',\n",
       "  'shouldnt',\n",
       "  'test',\n",
       "  'run',\n",
       "  'climat',\n",
       "  'chang',\n",
       "  'jackson',\n",
       "  'howev',\n",
       "  'eye',\n",
       "  'challeng',\n",
       "  'face',\n",
       "  'temperatur',\n",
       "  'rise',\n",
       "  'experi',\n",
       "  'covid',\n",
       "  'wake',\n",
       "  'thing',\n",
       "  'matter',\n",
       "  'china',\n",
       "  'health',\n",
       "  'month',\n",
       "  'experi',\n",
       "  'pandem',\n",
       "  'goldfuss',\n",
       "  'news',\n",
       "  'way',\n",
       "  'world',\n",
       "  'govern',\n",
       "  'jump',\n",
       "  'economi',\n",
       "  'opportun',\n",
       "  'one',\n",
       "  'dure',\n",
       "  'recess',\n",
       "  'obama',\n",
       "  'administr',\n",
       "  'rebuild',\n",
       "  'collaps',\n",
       "  'economi',\n",
       "  'climat',\n",
       "  'legisl',\n",
       "  'countri',\n",
       "  'histori',\n",
       "  'support',\n",
       "  'rebuild',\n",
       "  'countri',\n",
       "  'goldfuss',\n",
       "  'jackson',\n",
       "  'point',\n",
       "  'recess',\n",
       "  'today',\n",
       "  'footnot',\n",
       "  'fight',\n",
       "  'warm',\n",
       "  'impact',\n",
       "  'evid',\n",
       "  'challeng',\n",
       "  'carbon',\n",
       "  'emiss',\n",
       "  'climat',\n",
       "  'sever',\n",
       "  'econom',\n",
       "  'downturn',\n",
       "  'effect',\n",
       "  'wa',\n",
       "  'surprisingli',\n",
       "  'jackson',\n",
       "  'question',\n",
       "  'coronaviru',\n",
       "  'pandem',\n",
       "  'month',\n",
       "  'thing',\n",
       "  'fundament',\n",
       "  'economi',\n",
       "  'crash',\n",
       "  'way',\n",
       "  'becaus',\n",
       "  'opportun',\n",
       "  'energi',\n",
       "  'consumpt',\n",
       "  'goldfuss',\n",
       "  'recess',\n",
       "  'playbook',\n",
       "  'massiv',\n",
       "  'downturn',\n",
       "  'environment',\n",
       "  'sustain',\n",
       "  'way',\n",
       "  'time',\n",
       "  'occup',\n",
       "  'regard',\n",
       "  'climat',\n",
       "  'chang',\n",
       "  'hoax',\n",
       "  'possibl',\n",
       "  'becaus',\n",
       "  'presid',\n",
       "  'leadership',\n",
       "  'wa',\n",
       "  'vision',\n",
       "  'futur',\n",
       "  'goldfuss',\n",
       "  'trump',\n",
       "  'charg',\n",
       "  'rebuild',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'fuel',\n",
       "  'trump',\n",
       "  'ha',\n",
       "  'interest',\n",
       "  'transit',\n",
       "  's',\n",
       "  'carbon',\n",
       "  'base',\n",
       "  'economi',\n",
       "  'month',\n",
       "  'presid',\n",
       "  'depart',\n",
       "  'energi',\n",
       "  'oil',\n",
       "  'strateg',\n",
       "  'petroleum',\n",
       "  'reserv',\n",
       "  'price',\n",
       "  'taxpay',\n",
       "  'dollar',\n",
       "  'oil',\n",
       "  'industri',\n",
       "  'trump',\n",
       "  'consensu',\n",
       "  'climat',\n",
       "  'chang',\n",
       "  'caus',\n",
       "  'larg',\n",
       "  'burn',\n",
       "  'fuel',\n",
       "  'wors',\n",
       "  'window',\n",
       "  'opportun',\n",
       "  'term',\n",
       "  'hold',\n",
       "  'case',\n",
       "  'scenario',\n",
       "  'exceed',\n",
       "  'degre',\n",
       "  'death',\n",
       "  'exposur',\n",
       "  'number',\n",
       "  'burn',\n",
       "  'year',\n",
       "  'wildfir',\n",
       "  'peopl',\n",
       "  'risk',\n",
       "  'flood',\n",
       "  'level',\n",
       "  'rise',\n",
       "  'melt',\n",
       "  'greenland',\n",
       "  'ice',\n",
       "  'sheet',\n",
       "  'continu',\n",
       "  'worri',\n",
       "  'scientist',\n",
       "  'possibl',\n",
       "  'virus',\n",
       "  'permafrost',\n",
       "  'diseas',\n",
       "  'human',\n",
       "  'whatev',\n",
       "  'connect',\n",
       "  'coronaviru',\n",
       "  'climat',\n",
       "  'chang',\n",
       "  'jackson',\n",
       "  'analog',\n",
       "  'i',\n",
       "  'coronaviru',\n",
       "  'quit',\n",
       "  'while',\n",
       "  'end',\n",
       "  'exponenti',\n",
       "  'curv',\n",
       "  'look',\n",
       "  'anyth',\n",
       "  'jackson',\n",
       "  'part',\n",
       "  'curv',\n",
       "  'thing',\n",
       "  'start',\n",
       "  'past',\n",
       "  'project',\n",
       "  'futur',\n",
       "  'rise',\n",
       "  'temperatur',\n",
       "  'ha',\n",
       "  'repres',\n",
       "  'graph',\n",
       "  'describ',\n",
       "  'hockey',\n",
       "  'stick',\n",
       "  'yesterday',\n",
       "  'case',\n",
       "  'percent',\n",
       "  'case',\n",
       "  'weve',\n",
       "  'day',\n",
       "  'jackson',\n",
       "  'ad',\n",
       "  'part',\n",
       "  'curv',\n",
       "  'i',\n",
       "  'climat',\n",
       "  'scientist',\n",
       "  'frustrat',\n",
       "  'thing',\n",
       "  'thi',\n",
       "  'end',\n",
       "  'curv',\n",
       "  'thing',\n",
       "  'chang',\n",
       "  'year',\n",
       "  'weather',\n",
       "  'disast',\n",
       "  's',\n",
       "  'part',\n",
       "  'curv'],\n",
       " ['york',\n",
       "  'father',\n",
       "  'refus',\n",
       "  'age',\n",
       "  'son',\n",
       "  'nanuet',\n",
       "  'home',\n",
       "  'student',\n",
       "  'travel',\n",
       "  'island',\n",
       "  'spring',\n",
       "  'break',\n",
       "  'peter',\n",
       "  'levin',\n",
       "  'york',\n",
       "  'post',\n",
       "  'year',\n",
       "  'son',\n",
       "  'matt',\n",
       "  'home',\n",
       "  'earli',\n",
       "  'trip',\n",
       "  'avail',\n",
       "  'i',\n",
       "  'news',\n",
       "  'get',\n",
       "  'wors',\n",
       "  'wors',\n",
       "  'elder',\n",
       "  'levin',\n",
       "  'accord',\n",
       "  'year',\n",
       "  'father',\n",
       "  'hi',\n",
       "  'son',\n",
       "  'home',\n",
       "  'becaus',\n",
       "  'matt',\n",
       "  'grandpar',\n",
       "  'viru',\n",
       "  'sent',\n",
       "  'matt',\n",
       "  'hi',\n",
       "  'campu',\n",
       "  'bathroom',\n",
       "  'york',\n",
       "  'father',\n",
       "  'refus',\n",
       "  'year',\n",
       "  'son',\n",
       "  'home',\n",
       "  'springfield',\n",
       "  'colleg',\n",
       "  'student',\n",
       "  'travel',\n",
       "  'island',\n",
       "  'texa',\n",
       "  'spring',\n",
       "  'break',\n",
       "  'peter',\n",
       "  'levin',\n",
       "  'year',\n",
       "  'son',\n",
       "  'matt',\n",
       "  'vacat',\n",
       "  'earli',\n",
       "  'day',\n",
       "  'mayb',\n",
       "  'home',\n",
       "  'i',\n",
       "  'news',\n",
       "  'get',\n",
       "  'wors',\n",
       "  'wors',\n",
       "  'elder',\n",
       "  'levin',\n",
       "  'ad',\n",
       "  'pictur',\n",
       "  'friend',\n",
       "  'congreg',\n",
       "  'outdoor',\n",
       "  'music',\n",
       "  'accord',\n",
       "  'year',\n",
       "  'son',\n",
       "  'friend',\n",
       "  'couldnt',\n",
       "  'stay',\n",
       "  'famili',\n",
       "  'home',\n",
       "  'york',\n",
       "  'trip',\n",
       "  'plan',\n",
       "  'need',\n",
       "  'matt',\n",
       "  'hi',\n",
       "  'return',\n",
       "  'island',\n",
       "  'levin',\n",
       "  'son',\n",
       "  'chanc',\n",
       "  'airport',\n",
       "  'theyd',\n",
       "  'way',\n",
       "  'home',\n",
       "  'car',\n",
       "  'servic',\n",
       "  'car',\n",
       "  'driveway',\n",
       "  'i',\n",
       "  'year',\n",
       "  'group',\n",
       "  'return',\n",
       "  'matt',\n",
       "  'car',\n",
       "  'group',\n",
       "  'hour',\n",
       "  'drive',\n",
       "  'campu',\n",
       "  'massachusett',\n",
       "  'peter',\n",
       "  'levin',\n",
       "  'home',\n",
       "  'i',\n",
       "  'son',\n",
       "  'post',\n",
       "  'i',\n",
       "  'ani',\n",
       "  'matt',\n",
       "  'hi',\n",
       "  'father',\n",
       "  'trunk',\n",
       "  'hi',\n",
       "  'car',\n",
       "  'bag',\n",
       "  'groceri',\n",
       "  'envelop',\n",
       "  'driver',\n",
       "  'seat',\n",
       "  'matt',\n",
       "  'home',\n",
       "  'anytim',\n",
       "  'end',\n",
       "  'june',\n",
       "  'none',\n",
       "  'parent',\n",
       "  'peter',\n",
       "  'post']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = []\n",
    "processed_docs = news['nouns'].tolist()\n",
    "processed_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abov\n",
      "1 action\n",
      "2 activ\n",
      "3 ad\n",
      "4 administr\n",
      "5 agre\n",
      "6 agreement\n",
      "7 analog\n",
      "8 anyth\n",
      "9 author\n",
      "10 averag\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words appearing less than 10 times or in more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n= 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 13 (\"news\") appears 3 time.\n",
      "Word 14 (\"number\") appears 1 time.\n",
      "Word 15 (\"outbreak\") appears 1 time.\n",
      "Word 17 (\"point\") appears 4 time.\n",
      "Word 19 (\"presid\") appears 4 time.\n",
      "Word 25 (\"time\") appears 1 time.\n",
      "Word 26 (\"trump\") appears 15 time.\n",
      "Word 38 (\"crisi\") appears 1 time.\n",
      "Word 41 (\"march\") appears 1 time.\n",
      "Word 43 (\"respons\") appears 1 time.\n",
      "Word 46 (\"sinc\") appears 1 time.\n",
      "Word 48 (\"care\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 10,\n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.059*\"month\" + 0.050*\"i\" + 0.049*\"dure\" + 0.046*\"call\" + 0.045*\"world\" + 0.040*\"crisi\" + 0.038*\"thi\" + 0.035*\"nation\" + 0.034*\"possibl\" + 0.034*\"sinc\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.092*\"death\" + 0.084*\"sunday\" + 0.064*\"home\" + 0.062*\"china\" + 0.041*\"offici\" + 0.038*\"number\" + 0.034*\"restrict\" + 0.034*\"citi\" + 0.030*\"confirm\" + 0.027*\"place\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.099*\"time\" + 0.075*\"worker\" + 0.063*\"work\" + 0.054*\"home\" + 0.052*\"economi\" + 0.045*\"spread\" + 0.042*\"s\" + 0.042*\"share\" + 0.034*\"thi\" + 0.032*\"hospit\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.123*\"s\" + 0.080*\"nation\" + 0.080*\"presid\" + 0.078*\"unit\" + 0.049*\"trump\" + 0.035*\"month\" + 0.030*\"call\" + 0.028*\"effort\" + 0.025*\"author\" + 0.023*\"becaus\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.086*\"death\" + 0.086*\"hospit\" + 0.085*\"offici\" + 0.058*\"posit\" + 0.056*\"covid\" + 0.056*\"system\" + 0.037*\"york\" + 0.031*\"march\" + 0.031*\"saturday\" + 0.030*\"nation\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.078*\"ad\" + 0.071*\"test\" + 0.052*\"author\" + 0.038*\"number\" + 0.038*\"china\" + 0.033*\"march\" + 0.031*\"point\" + 0.026*\"travel\" + 0.023*\"person\" + 0.022*\"hospit\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.113*\"hospit\" + 0.103*\"york\" + 0.078*\"home\" + 0.063*\"order\" + 0.058*\"i\" + 0.033*\"person\" + 0.028*\"news\" + 0.028*\"spread\" + 0.025*\"place\" + 0.025*\"way\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.084*\"test\" + 0.057*\"world\" + 0.052*\"offici\" + 0.048*\"home\" + 0.046*\"saturday\" + 0.043*\"govern\" + 0.039*\"death\" + 0.035*\"way\" + 0.033*\"minist\" + 0.029*\"covid\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.179*\"trump\" + 0.076*\"presid\" + 0.056*\"economi\" + 0.050*\"time\" + 0.042*\"point\" + 0.038*\"crisi\" + 0.038*\"i\" + 0.035*\"outbreak\" + 0.034*\"news\" + 0.033*\"part\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.114*\"death\" + 0.082*\"govern\" + 0.082*\"hospit\" + 0.050*\"nation\" + 0.050*\"system\" + 0.050*\"media\" + 0.034*\"concern\" + 0.034*\"citi\" + 0.034*\"test\" + 0.034*\"news\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.059*\"month\" + 0.050*\"i\" + 0.049*\"dure\" + 0.046*\"call\" + 0.045*\"world\" + 0.040*\"crisi\" + 0.038*\"thi\" + 0.035*\"nation\" + 0.034*\"possibl\" + 0.034*\"sinc\"'),\n",
       " (1,\n",
       "  '0.092*\"death\" + 0.084*\"sunday\" + 0.064*\"home\" + 0.062*\"china\" + 0.041*\"offici\" + 0.038*\"number\" + 0.034*\"restrict\" + 0.034*\"citi\" + 0.030*\"confirm\" + 0.027*\"place\"'),\n",
       " (2,\n",
       "  '0.099*\"time\" + 0.075*\"worker\" + 0.063*\"work\" + 0.054*\"home\" + 0.052*\"economi\" + 0.045*\"spread\" + 0.042*\"s\" + 0.042*\"share\" + 0.034*\"thi\" + 0.032*\"hospit\"'),\n",
       " (3,\n",
       "  '0.123*\"s\" + 0.080*\"nation\" + 0.080*\"presid\" + 0.078*\"unit\" + 0.049*\"trump\" + 0.035*\"month\" + 0.030*\"call\" + 0.028*\"effort\" + 0.025*\"author\" + 0.023*\"becaus\"'),\n",
       " (4,\n",
       "  '0.086*\"death\" + 0.086*\"hospit\" + 0.085*\"offici\" + 0.058*\"posit\" + 0.056*\"covid\" + 0.056*\"system\" + 0.037*\"york\" + 0.031*\"march\" + 0.031*\"saturday\" + 0.030*\"nation\"'),\n",
       " (5,\n",
       "  '0.078*\"ad\" + 0.071*\"test\" + 0.052*\"author\" + 0.038*\"number\" + 0.038*\"china\" + 0.033*\"march\" + 0.031*\"point\" + 0.026*\"travel\" + 0.023*\"person\" + 0.022*\"hospit\"'),\n",
       " (6,\n",
       "  '0.113*\"hospit\" + 0.103*\"york\" + 0.078*\"home\" + 0.063*\"order\" + 0.058*\"i\" + 0.033*\"person\" + 0.028*\"news\" + 0.028*\"spread\" + 0.025*\"place\" + 0.025*\"way\"'),\n",
       " (7,\n",
       "  '0.084*\"test\" + 0.057*\"world\" + 0.052*\"offici\" + 0.048*\"home\" + 0.046*\"saturday\" + 0.043*\"govern\" + 0.039*\"death\" + 0.035*\"way\" + 0.033*\"minist\" + 0.029*\"covid\"'),\n",
       " (8,\n",
       "  '0.179*\"trump\" + 0.076*\"presid\" + 0.056*\"economi\" + 0.050*\"time\" + 0.042*\"point\" + 0.038*\"crisi\" + 0.038*\"i\" + 0.035*\"outbreak\" + 0.034*\"news\" + 0.033*\"part\"'),\n",
       " (9,\n",
       "  '0.114*\"death\" + 0.082*\"govern\" + 0.082*\"hospit\" + 0.050*\"nation\" + 0.050*\"system\" + 0.050*\"media\" + 0.034*\"concern\" + 0.034*\"citi\" + 0.034*\"test\" + 0.034*\"news\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 10)])\n",
    "\n",
    "topics_df = pd.DataFrame(top_words_per_topic, columns=['topic', 'word', 'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>word</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>month</td>\n",
       "      <td>0.059162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "      <td>0.049670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dure</td>\n",
       "      <td>0.048573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>call</td>\n",
       "      <td>0.046067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>world</td>\n",
       "      <td>0.045216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9</td>\n",
       "      <td>media</td>\n",
       "      <td>0.049780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9</td>\n",
       "      <td>concern</td>\n",
       "      <td>0.034052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>9</td>\n",
       "      <td>citi</td>\n",
       "      <td>0.033943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>0.033640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>9</td>\n",
       "      <td>news</td>\n",
       "      <td>0.033583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic     word         p\n",
       "0       0    month  0.059162\n",
       "1       0        i  0.049670\n",
       "2       0     dure  0.048573\n",
       "3       0     call  0.046067\n",
       "4       0    world  0.045216\n",
       "..    ...      ...       ...\n",
       "95      9    media  0.049780\n",
       "96      9  concern  0.034052\n",
       "97      9     citi  0.033943\n",
       "98      9     test  0.033640\n",
       "99      9     news  0.033583\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_ct = pd.crosstab(index=[topics_df.topic], \n",
    "                           columns = topics_df.word, \n",
    "                           values=topics_df.p, \n",
    "                                  aggfunc=pd.Series.sum\n",
    "                           ).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>topic</th>\n",
       "      <th>ad</th>\n",
       "      <th>author</th>\n",
       "      <th>becaus</th>\n",
       "      <th>call</th>\n",
       "      <th>china</th>\n",
       "      <th>citi</th>\n",
       "      <th>concern</th>\n",
       "      <th>confirm</th>\n",
       "      <th>covid</th>\n",
       "      <th>...</th>\n",
       "      <th>thi</th>\n",
       "      <th>time</th>\n",
       "      <th>travel</th>\n",
       "      <th>trump</th>\n",
       "      <th>unit</th>\n",
       "      <th>way</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>world</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.046067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061841</td>\n",
       "      <td>0.033756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034265</td>\n",
       "      <td>0.099097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062846</td>\n",
       "      <td>0.074678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.02344</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049413</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.077642</td>\n",
       "      <td>0.051889</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057272</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033943</td>\n",
       "      <td>0.034052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word  topic        ad    author   becaus      call     china      citi  \\\n",
       "0         0  0.000000  0.000000  0.00000  0.046067  0.000000  0.000000   \n",
       "1         1  0.000000  0.000000  0.00000  0.000000  0.061841  0.033756   \n",
       "2         2  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "3         3  0.000000  0.024507  0.02344  0.030296  0.000000  0.000000   \n",
       "4         4  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "5         5  0.077642  0.051889  0.00000  0.000000  0.037898  0.000000   \n",
       "6         6  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "7         7  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "8         8  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "9         9  0.000000  0.000000  0.00000  0.000000  0.000000  0.033943   \n",
       "\n",
       "word   concern   confirm     covid  ...       thi      time    travel  \\\n",
       "0     0.000000  0.000000  0.000000  ...  0.037920  0.000000  0.000000   \n",
       "1     0.000000  0.029897  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  ...  0.034265  0.099097  0.000000   \n",
       "3     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.056429  ...  0.000000  0.000000  0.000000   \n",
       "5     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.025916   \n",
       "6     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7     0.000000  0.000000  0.028561  ...  0.000000  0.000000  0.000000   \n",
       "8     0.000000  0.000000  0.000000  ...  0.000000  0.050002  0.000000   \n",
       "9     0.034052  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "word     trump      unit       way      work    worker     world      york  \n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.045216  0.000000  \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2     0.000000  0.000000  0.000000  0.062846  0.074678  0.000000  0.000000  \n",
       "3     0.049413  0.077917  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.037081  \n",
       "5     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6     0.000000  0.000000  0.024649  0.000000  0.000000  0.000000  0.102561  \n",
       "7     0.000000  0.000000  0.035404  0.000000  0.000000  0.057272  0.000000  \n",
       "8     0.179059  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 55 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_ct.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_tots = pd.crosstab( index=1,\n",
    "                           columns = topics_df.word, \n",
    "                           values=topics_df.p, \n",
    "                                  aggfunc=pd.Series.sum\n",
    "                           ).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_tots = topics_tots.add_prefix(\"ldan_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_tots.drop(columns=['ldan_row_0']).to_parquet('yahoo_top_news_ldan_topics.' + today_dt + '.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
