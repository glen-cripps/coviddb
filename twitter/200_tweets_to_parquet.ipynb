{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file u'tweets.20200329.json', mode 'r' at 0x7fdc4b1ef0c0>\n"
     ]
    }
   ],
   "source": [
    "# %load 200_tweets_to_parquet.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Mon Sep 23 13:52:14 2019\n",
    "\n",
    "@author: hduser\n",
    "\"\"\"\n",
    "import time\n",
    "import arrow\n",
    "today_dt = arrow.now().format('YYYYMMDD')\n",
    "\n",
    "import tarfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Comment out code that doesnt need to run every time\n",
    "#tf = tarfile.open(\"geotagged_tweets_20160812-0912.tar.gz\")\n",
    "#tf.extractall()\n",
    "\n",
    "#os.system(\"shuf -n 1000 geotagged_tweets_20160812-0912.jsons > geotagged_tweets_20160812-0912.1000.jsons\")\n",
    "#os.system(\"shuf -n 5000 geotagged_tweets_20160812-0912.jsons > geotagged_tweets_20160812-0912.5000.jsons\")\n",
    "#os.system(\"shuf -n 10000 geotagged_tweets_20160812-0912.jsons > geotagged_tweets_20160812-0912.10000.jsons\")\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open('tweets.' + today_dt + '.json', \"r\")\n",
    "print(tweets_file)\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "counter = 0\n",
    "cols = ['created_at','place.country','place.country_code','place.name','is_quote_status','lang','source','text','user.created_at','user.description','user.name','user.location','place.place_type','user.screen_name','in_reply_to_screen_name']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in tweets_file:\n",
    "    counter = counter + 1\n",
    "    if counter % 10000 == 0:\n",
    "        print(counter)\n",
    "        tweets_df = json_normalize(tweets_data)\n",
    "        neat_df = tweets_df #[cols]\n",
    "        neat_df.to_parquet(\"tweets.{}.parquet\".format(counter), compression='GZIP')\n",
    "        tweets_data = []\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "tweets_df = json_normalize(tweets_data)\n",
    "neat_df = tweets_df #[cols]\n",
    "neat_df.to_parquet(\"tweets.{}.parquet\".format(counter),compression='GZIP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of program\n",
      "['tweets.3110.parquet', 'tweets.556.parquet']\n"
     ]
    }
   ],
   "source": [
    "# This creates a python list of strings with json data in the string.  there's a lot of fields, and I can either\n",
    "# cherry pick the fields I want, or figure out how to read them all in and subset them once I have them in a nice dataframe\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "\n",
    "\n",
    "print(\"Start of program\")\n",
    "\n",
    "path = ''                     # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"tweets.*.parquet\"))     # advisable to use os.path.join as this makes concatenation \n",
    "\n",
    "all_files.sort()\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file read tweets.3110.parquet\n",
      "                       created_at place.country place.country_code place.name  \\\n",
      "0  Sun Mar 29 14:49:56 +0000 2020          None               None       None   \n",
      "1  Sun Mar 29 14:49:56 +0000 2020          None               None       None   \n",
      "2  Sun Mar 29 14:49:56 +0000 2020          None               None       None   \n",
      "3  Sun Mar 29 14:49:56 +0000 2020          None               None       None   \n",
      "4  Sun Mar 29 14:49:56 +0000 2020          None               None       None   \n",
      "\n",
      "   is_quote_status lang                                             source  \\\n",
      "0              0.0   es  <a href=\"http://twitter.com/download/iphone\" r...   \n",
      "1              1.0   en  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
      "2              0.0   es  <a href=\"http://twitter.com/download/android\" ...   \n",
      "3              0.0   in  <a href=\"http://twitter.com/download/iphone\" r...   \n",
      "4              0.0   es  <a href=\"http://twitter.com/download/iphone\" r...   \n",
      "\n",
      "                                                text  \\\n",
      "0  RT @teresitaradiol: Los mexicanos no creen en ...   \n",
      "1  RT @iamkarendavila: TIME IS OF THE ESSENCE. Dr...   \n",
      "2  RT @miguelhotero: Curioso que en Cuba y Venezu...   \n",
      "3  RT @aainaaqila: Scammer: Saya call dari SPRM, ...   \n",
      "4  RT @cesarlitardo: El covid, le gano la batalla...   \n",
      "\n",
      "                  user.created_at  \\\n",
      "0  Wed Feb 27 21:39:34 +0000 2013   \n",
      "1  Wed Dec 19 23:54:55 +0000 2018   \n",
      "2  Sat Oct 19 01:47:22 +0000 2013   \n",
      "3  Sat Nov 21 07:07:41 +0000 2015   \n",
      "4  Mon Jun 07 17:29:12 +0000 2010   \n",
      "\n",
      "                                    user.description             user.name  \\\n",
      "0  Crimmigration strategist. Power to the people....                    SV   \n",
      "1  Christian - Lutheran\\n\\n\\n\\n\\nEach day is a gi...                   ...   \n",
      "2  Ing quimico ULA,amante de mi esposa padre de m...           Argentino Y   \n",
      "3                                                 :(  kerol #DudukRumahMak   \n",
      "4                    Abogada Laboralista de UniNorte     Tatiana Barrios A   \n",
      "\n",
      "               user.location place.place_type user.screen_name  \\\n",
      "0                       None             None       arreenitta   \n",
      "1                       None             None         eysibi12   \n",
      "2                       None             None         afryka53   \n",
      "3      Kota Kinabalu, Perlis             None      khairidzwan   \n",
      "4  ÃœT: 9.2968493,-75.3906836             None  TatianaBAlvarez   \n",
      "\n",
      "  in_reply_to_screen_name  \n",
      "0                    None  \n",
      "1                    None  \n",
      "2                    None  \n",
      "3                    None  \n",
      "4                    None  \n",
      "first iter... initialize stocks dataframe with all the data in this chunk\n",
      "file read tweets.556.parquet\n",
      "                       created_at place.country place.country_code place.name  \\\n",
      "0  Sun Mar 29 13:26:06 +0000 2020          None               None       None   \n",
      "1  Sun Mar 29 13:26:06 +0000 2020          None               None       None   \n",
      "2  Sun Mar 29 13:26:06 +0000 2020          None               None       None   \n",
      "3  Sun Mar 29 13:26:06 +0000 2020          None               None       None   \n",
      "4  Sun Mar 29 13:26:06 +0000 2020          None               None       None   \n",
      "\n",
      "   is_quote_status lang                                             source  \\\n",
      "0              0.0   es  <a href=\"http://twitter.com/download/android\" ...   \n",
      "1              0.0   es  <a href=\"http://twitter.com/download/android\" ...   \n",
      "2              1.0   en  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
      "3              1.0  und  <a href=\"http://twitter.com/download/iphone\" r...   \n",
      "4              1.0   es  <a href=\"http://twitter.com/download/android\" ...   \n",
      "\n",
      "                                                text  \\\n",
      "0  Esperemos hoy se un mejor dÃ­a para todos los e...   \n",
      "1  RT @rcnradio: MedellÃ­n confirma el tercer paci...   \n",
      "2  Oh my God! ðŸ˜± I can't believe there have been a...   \n",
      "3                                                WOW   \n",
      "4  RT @Kualy2014: #Cuba Raciona los alimentos por...   \n",
      "\n",
      "                  user.created_at  \\\n",
      "0  Fri Jul 01 18:15:26 +0000 2011   \n",
      "1  Wed Feb 10 00:33:59 +0000 2016   \n",
      "2  Sat Oct 05 17:12:50 +0000 2019   \n",
      "3  Mon Apr 09 03:46:31 +0000 2012   \n",
      "4  Wed Dec 24 18:06:38 +0000 2014   \n",
      "\n",
      "                                    user.description  \\\n",
      "0  Abogado en libre ejercicio. Piloto, corredor, ...   \n",
      "1                        NO  COMUNISMO NO CORRUPCIÃ“N   \n",
      "2  ðŸ˜ŽðŸ†’â˜¯ï¸ Everyone in America ðŸ‡ºðŸ‡¸ please all vote fo...   \n",
      "3  The name is Gary Johnson From Weatherford! R.I...   \n",
      "4                         ðŸ“¢ El mundo se autodestruye   \n",
      "\n",
      "                          user.name user.location place.place_type  \\\n",
      "0              Fausto AdriÃ¡n GarcÃ­a      Riobamba             None   \n",
      "1                           Tomas V          None             None   \n",
      "2  Daisy Cham ðŸ§¢ðŸ§£â˜‚ï¸ðŸŒ‚ðŸ˜·ðŸ¥½â›‘ðŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ðŸ‡¬ðŸ‡§ðŸ‡­ðŸ‡°ðŸ‡³ðŸ‡¬  Macclesfield             None   \n",
      "3                           GJ Snow          None             None   \n",
      "4                         Curiosity          None             None   \n",
      "\n",
      "  user.screen_name in_reply_to_screen_name  \n",
      "0    Faustoadriang                    None  \n",
      "1           VYLYVY                    None  \n",
      "2       daizy_cham                    None  \n",
      "3      GJ_TheKid17                    None  \n",
      "4        Kualy2014                    None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i,f in enumerate(all_files):\n",
    "    parquet_chunk = pd.read_parquet(f)\n",
    "    print(\"file read \" + f)\n",
    "    print(parquet_chunk.head())\n",
    "    if i == 0:\n",
    "        print(\"first iter... initialize stocks dataframe with all the data in this chunk\")\n",
    "        tweets = parquet_chunk\n",
    "    else:\n",
    "        tweets = tweets.append(parquet_chunk, ignore_index=True)\n",
    "\n",
    "tweets.to_parquet(\"tweets_all.\" + today_dt + \".parquet\",compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (system-wide)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
